
\documentclass[16pt,UTF8]{ctexart}
\usepackage{indentfirst}
\setlength{\parindent}{2em}

\begin{document}\LARGE
\section{神经网络}
\subsection{softmax，交叉熵}
	\begin{displaymath}
	Softmax\textrm{回归}:softmax(y)_{i} = e^{yi}/\sum_{j=1}^n e^{yi}	
	\end{displaymath}
	Softmax层将神经网络的输出变成一个概率分布，从而可以通过交叉熵来计算预测的概率分布和真实答案的概率分布之间的距离了。\\
	\begin{displaymath}
	H(p,q) = \sum_{x} p(x)log(q(x)) 
	\end{displaymath}
	交叉熵刻画的是两个概率分布之间的距离，他是不对称的。
	\begin{displaymath}
  	H(p,q) \neq H(q,p)
	\end{displaymath}
	他刻画的是通过概率分布q来表达概率分布p的困难程度。因为正确答案是希望得到的结果，所以当交叉熵作为神经网络的损失函数的时候，p代表的是正确答案，q代表的师预测值。交叉熵刻画的是两个概率分布的距离，也就是说交叉熵越小，两个概率分布越接近。
	
\subsection{L1,L2正则化}
	\indent L1:Lasso , L2:Ridge \\		

	\begin{displaymath}
	L1:R(w) = ||w||_{1} = \sum_{i}|w_{i}|
	\end{displaymath}
	\begin{displaymath}
	L2:R(w) = ||w||_{2} = \sum_{i}|w_{i}^{2}|
	\end{displaymath}
	
	\indent L1:让参数更稀疏，L2则不会。因为当参数很小时，比如0.001，这个参数的平方基本上就可以忽略了，于是模型不会进一步减个这个参数调整为0。\\
	\indent L1不可导而L2可导。因为在优化的时候需要计算损失函数的偏导数，所以对L2正则化的损失函数的优化更加简洁，优化L1正则化的损失函数更加复杂。\\
	\indent 实践中也可将L1和L2同时使用:
	\begin{displaymath}
		R(w) = \sum_{i}\alpha|w_{i}| + (1 - \alpha)w_{i}^{2}
	\end{displaymath}
	
\subsection{神经网络优化过程中可能遇到的问题}
	\begin{enumerate}
	\item 通过指数衰减的方式来设置学习率。通过这种方法，既可以加快训练初期的训练速度，同时在训练后期又不会出现损失函数在极小值附近徘徊往返的情况。
	\item 通过正则化解决过拟合的问题。
	\item 使用滑动平均模型让最后得到的模型在未知数据上更健壮了。
	\end{enumerate}

\section{卷积神经网络}
	\indent 经典图片分类问题的卷积神经网络结构:\\输入层 -> (卷积层->池化层)? -> 全连接层\\
	\indent 卷积层之间往往会加上一个池化层(pooling layer),池化层可以非常有效的所系哦啊矩阵尺寸，从而减少最后全连接层中的参数，加快计算速度并且防止过拟合。\\
	\indent 常用max pooling ,其次 average pooling 。\\
	\indent 一般卷积层的过滤器边长不会超过5，但有些卷积神经网络结构中，处理输入的卷积层中使用了边长为7甚至11的过滤器。\\
	\indent 在过滤器的深度上，大部分卷积神经网络都采用了逐层递增的方式。卷积层的步长一般为1，但是有些模型中会选择2或者3作为步长，池化层最多用的是最大池化层，池化层过滤器的边长一般为2或者3，步长也一般为2或者3。

	
\section{循环神经网络}
	\subsection{LSTM：长短时记忆网络}
	
	
	
	
	
	
	
	
	
\end{document}
